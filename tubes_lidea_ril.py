# -*- coding: utf-8 -*-
"""Tubes-lidea-ril.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1v8FMiDC6Vmr8u-fzLf-eSjCF4p_t8bS8

# Import Dependecies
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from scipy.stats import zscore
from scipy import stats

"""# Load Data"""

!pip install gdown
!gdown --fuzzy "https://docs.google.com/spreadsheets/d/1mhDBGeIt2b6-FFKrolccp6Jog7ZvhrZN/edit?usp=sharing&ouid=116842443111305752575&rtpof=true&sd=true"

df_raw = pd.read_excel("1744774003471_Data_Tugas_Besar_FINAL.xlsx")
df_raw.head()

df_raw.shape

"""# Data Wrangling"""

# Hapus kolom yang tidak diperlukan
df = df_raw.copy()
df.drop(columns=['DATE_NO', 'TAG', 'Amount Comp_01', 'NAME'], inplace=True, errors='ignore')

# Pivot: setiap kolom adalah satu DESCRIPTION, isinya VALUE, baris berdasarkan DATE_TIME
pivot_df = df.pivot(index='DATE_TIME', columns='DESCRIPTION', values='VALUE')

# Ambil hanya 640 baris pertama dari index 0
pivot_df = pivot_df.iloc[0:640]

# Simpan ke file Excel baru
output_pivot = "pivot_55_description_640rows.xlsx"
pivot_df.to_excel(output_pivot)

print(f"Data pivot dengan 55 kolom DESCRIPTION dan 640 baris DATE_TIME telah disimpan di: {output_pivot}")

df = pd.read_excel('pivot_55_description_640rows.xlsx')
df.head()

# Convert DATE_TIME to datetime
df['DATE_TIME'] = pd.to_datetime(df['DATE_TIME'], format='%Y%m%d')

df.shape

"""# DATA CLEANING"""

# prompt: Hapus nilai outlier berdasarkan tiap kolom menggunakan metode iqr berdasarkan parameter kolom "DATE_TIME"

# Calculate IQR for each column except 'DATE_TIME'
Q1 = df.drop(columns=['DATE_TIME']).quantile(0.25)
Q3 = df.drop(columns=['DATE_TIME']).quantile(0.75)
IQR = Q3 - Q1

# Define bounds for outliers
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

# Initialize a boolean mask with all values as True
outlier_mask = pd.Series(True, index=df.index)

# Iterate through each column (excluding 'DATE_TIME') to create the mask
for col in df.columns.drop('DATE_TIME'):
    col_outlier_mask = (df[col] >= lower_bound[col]) & (df[col] <= upper_bound[col])
    outlier_mask = outlier_mask & col_outlier_mask

# Filter the DataFrame based on the combined outlier mask
df_cleaned = df[outlier_mask].copy()

print(f"Original shape: {df.shape}")
print(f"Shape after removing outliers: {df_cleaned.shape}")

# # Function to remove outliers using z-score
# def remove_outliers(df, columns, z_threshold=3):
#     for col in columns:
#         if df[col].dtype in ['int64', 'float64']:
#             z_scores = np.abs(stats.zscore(df[col], nan_policy='omit'))
#             df.loc[z_scores > z_threshold, col] = np.nan
#     return df

# Get numerical columns excluding DATE_TIME
numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns

# # Remove outliers
# df = remove_outliers(df, numerical_cols)

# Interpolate missing values
df = df.interpolate(method='linear', limit_direction='both')

# Replace remaining zeros with interpolated values
for col in numerical_cols:
    mask = df[col] == 0
    df.loc[mask, col] = np.nan
    df[col] = df[col].interpolate(method='linear', limit_direction='both')

# Show results
print("Shape after cleaning:", df.shape)
print("\nMissing values after interpolation:", df.isnull().sum().sum())
print("\nSample of cleaned data:")
print(df.head())

df.shape

df.head()

df.shape

# Key variables for each well
key_variables = ['91_9500062414', '91_EP_NATURAL_GAS', 'ANPR', 'C_PR', 'FBHP', 'ANTP', 'C_TE', 'FBHT', 'C_OP']

# Iterate through each well
for well in ['AA1', 'AA2', 'AA3', 'AA4', 'AA5']:
    print(f"\n--- Summary Statistics for {well} ---")
    for var in key_variables:
        column_name = f'{well}_{var}'
        print(f"\n--- Summary Statistics for {column_name} ---")
        print(df[column_name].describe())
        plt.figure(figsize=(12, 6))
        plt.subplot(1, 2, 1)
        sns.histplot(df[column_name], kde=True)
        plt.title(f'Distribution of {column_name}')
        plt.subplot(1, 2, 2)
        sns.boxplot(y=df[column_name])
        plt.title(f'Boxplot of {column_name}')
        plt.tight_layout()
        plt.show()

# prompt: buatkan aku grafik histogram untuk semua kolom

df.hist(bins=50, figsize=(20, 15))
plt.tight_layout()
plt.show()

# prompt: buat grafik berdasarkan waktunya ("DATE_TIME") untuk semua kolom

# Buat grafik berdasarkan waktu untuk semua kolom numerik
numerical_cols = df.select_dtypes(include=np.number).columns

for col in numerical_cols:
    plt.figure(figsize=(12, 6))
    plt.plot(df['DATE_TIME'], df[col])
    plt.title(f'Graph of {col} over Time')
    plt.xlabel('DATE_TIME')
    plt.ylabel(col)
    plt.grid(True)
    plt.show()

# prompt: Hapus untuk semua kolom yang masih masuk ke outlier di boxplot
key_variables = ['91_9500062414', '91_EP_NATURAL_GAS', 'ANPR', 'C_PR', 'FBHP', 'ANTP', 'C_TE', 'FBHT', 'C_OP']

# Function to remove outliers using IQR
def remove_outliers_iqr(df, columns):
    for col in columns:
        if df[col].dtype in ['int64', 'float64']:
            Q1 = df[col].quantile(0.25)
            Q3 = df[col].quantile(0.75)
            IQR = Q3 - Q1
            lower_bound = Q1 - 1.5 * IQR
            upper_bound = Q3 + 1.5 * IQR
            df = df[(df[col] >= lower_bound) & (df[col] <= upper_bound)]
    return df

# Get numerical columns excluding DATE_TIME
numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()

# Remove outliers from the DataFrame based on the key variables for each well
for well in ['AA1', 'AA2', 'AA3', 'AA4', 'AA5']:
    print(f"Removing outliers for well: {well}")
    well_cols = [f'{well}_{var}' for var in key_variables if f'{well}_{var}' in numerical_cols]
    df = remove_outliers_iqr(df, well_cols)

print("\nShape after removing outliers based on key variables:", df.shape)
print("\nSample of data after removing outliers:")
print(df.head())

df.shape

# prompt: lalu isi nilai outlier yang telah dihapus menggunakan metode interpolasi hingga jumlah barisnya sama dengan data sebelumnya

# The shape of the original DataFrame is 640 rows.
original_shape = 640

# Reindex the DataFrame after outlier removal to restore the original index range.
# This creates NaNs where rows were removed.
df_reindexed = df_cleaned.reindex(range(original_shape))

# Interpolate the missing values (NaNs) created by reindexing.
# This fills the gaps using linear interpolation.
df_interpolated = df_reindexed.interpolate(method='linear', limit_direction='both')

print(f"Shape after interpolation: {df_interpolated.shape}")
print("\nMissing values after interpolation:", df_interpolated.isnull().sum().sum())
print("\nSample of data after interpolation:")
print(df_interpolated.head())

# Now df_interpolated has the same number of rows as the original df (640)
# and the removed outlier values have been replaced by interpolated values.

# Key variables for each well
key_variables = ['91_9500062414', '91_EP_NATURAL_GAS', 'ANPR', 'C_PR', 'FBHP', 'ANTP', 'C_TE', 'FBHT', 'C_OP']

# Iterate through each well
for well in ['AA1', 'AA2', 'AA3', 'AA4', 'AA5']:
    print(f"\n--- Summary Statistics for {well} ---")
    for var in key_variables:
        column_name = f'{well}_{var}'
        print(f"\n--- Summary Statistics for {column_name} ---")
        print(df[column_name].describe())
        plt.figure(figsize=(12, 6))
        plt.subplot(1, 2, 1)
        sns.histplot(df[column_name], kde=True)
        plt.title(f'Distribution of {column_name}')
        plt.subplot(1, 2, 2)
        sns.boxplot(y=df[column_name])
        plt.title(f'Boxplot of {column_name}')
        plt.tight_layout()
        plt.show()

df.shape

# Iterate through each well
for well in ['AA1', 'AA2', 'AA3', 'AA4', 'AA5']:
    well_data = df[[f'{well}_{var}' for var in key_variables]]
    correlation_matrix = well_data.corr()
    plt.figure(figsize=(10, 8))
    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
    plt.title(f'Correlation Matrix for {well}')
    plt.savefig(f'{well}_heatmap.png') # Save the heatmap
    plt.show()

"""# Making a model"""

# Import necessary libraries
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.metrics import mean_squared_error, r2_score
import xgboost as xgb
from sklearn.svm import SVR

# Function to create and evaluate models for each well
def evaluate_models(X, y, well_name):
    # Split the data
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # Scale the features
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    # Initialize models
    models = {
        'Linear Regression': LinearRegression(),
        'Random Forest': RandomForestRegressor(random_state=42),
        'XGBoost': xgb.XGBRegressor(random_state=42),
        'Gradient Boosting': GradientBoostingRegressor(random_state=42),
        'SVR': SVR(kernel='rbf')
    }

    results = {}

    # Train and evaluate each model
    for name, model in models.items():
        model.fit(X_train_scaled, y_train)
        y_pred = model.predict(X_test_scaled)
        mse = mean_squared_error(y_test, y_pred)
        rmse = np.sqrt(mse)
        r2 = r2_score(y_test, y_pred)

        # Cross validation score
        cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5, scoring='r2')

        results[name] = {
            'RMSE': rmse,
            'R2': r2,
            'CV_R2_mean': cv_scores.mean(),
            'CV_R2_std': cv_scores.std(),
            'model': model,
            'scaler': scaler
        }

    print(f"\nResults for {well_name}:")
    for name, metrics in results.items():
        print(f"\n{name}:")
        print(f"RMSE: {metrics['RMSE']:.4f}")
        print(f"R2 Score: {metrics['R2']:.4f}")
        print(f"Cross-val R2: {metrics['CV_R2_mean']:.4f} (+/- {metrics['CV_R2_std']*2:.4f})")

    return results

# Prepare data for each well
wells = {
    'AA1': {
        'features': ['AA1_91_9500062414', 'AA1_ANPR', 'AA1_ANTP', 'AA1_C_OP', 'AA1_C_PR',
                    'AA1_C_TE', 'AA1_FBHP', 'AA1_FBHT', 'AA1_FTHP', 'AA1_FTHT'],
        'target': 'AA1_91_EP_NATURAL_GAS'
    },
    'AA2': {
        'features': ['AA2_91_9500062414', 'AA2_ANPR', 'AA2_ANTP', 'AA2_C_OP', 'AA2_C_PR',
                    'AA2_C_TE', 'AA2_FBHP', 'AA2_FBHT', 'AA2_FTHP', 'AA2_FTHT'],
        'target': 'AA2_91_EP_NATURAL_GAS'
    },
    'AA3': {
        'features': ['AA3_91_9500062414', 'AA3_ANPR', 'AA3_ANTP', 'AA3_C_OP', 'AA3_C_PR',
                    'AA3_C_TE', 'AA3_FBHP', 'AA3_FBHT', 'AA3_FTHP', 'AA3_FTHT'],
        'target': 'AA3_91_EP_NATURAL_GAS'
    },
    'AA4': {
        'features': ['AA4_91_9500062414', 'AA4_ANPR', 'AA4_ANTP', 'AA4_C_OP', 'AA4_C_PR',
                    'AA4_C_TE', 'AA4_FBHP', 'AA4_FBHT', 'AA4_FTHP', 'AA4_FTHT'],
        'target': 'AA4_91_EP_NATURAL_GAS'
    },
    'AA5': {
        'features': ['AA5_91_9500062414', 'AA5_ANPR', 'AA5_ANTP', 'AA5_C_OP', 'AA5_C_PR',
                    'AA5_C_TE', 'AA5_FBHP', 'AA5_FBHT', 'AA5_FTHP', 'AA5_FTHT'],
        'target': 'AA5_91_EP_NATURAL_GAS'
    }
}

# Train and evaluate models for each well
all_results = {}
for well_name, well_data in wells.items():
    X = df[well_data['features']]
    y = df[well_data['target']]
    all_results[well_name] = evaluate_models(X, y, well_name)

# prompt: Buatkan grafik yang dapat melihatkan grafik prediksi dan data aktual dengan hasil RMSE paling kecil dan r^2 paling besar

# Function to find the best model based on RMSE and R2
def find_best_model(results):
    best_model_name = None
    best_rmse = float('inf')
    best_r2 = -float('inf')

    for name, metrics in results.items():
        if metrics['RMSE'] < best_rmse and metrics['R2'] > best_r2:
            best_rmse = metrics['RMSE']
            best_r2 = metrics['R2']
            best_model_name = name
        elif metrics['RMSE'] == best_rmse and metrics['R2'] > best_r2:
            best_r2 = metrics['R2']
            best_model_name = name
        elif metrics['RMSE'] < best_rmse and metrics['R2'] == best_r2:
             best_rmse = metrics['RMSE']
             best_model_name = name


    return best_model_name

# Function to plot actual vs predicted values for the best model
def plot_best_model_prediction(df, well_name, best_model_name, model_info):
    X = df[wells[well_name]['features']]
    y = df[wells[well_name]['target']]

    # Use the trained model and scaler from the results
    best_model = model_info['model']
    scaler = model_info['scaler']

    # Scale the features
    X_scaled = scaler.transform(X)

    # Get predictions on the entire dataset (for plotting)
    y_pred = best_model.predict(X_scaled)

    plt.figure(figsize=(15, 7))
    plt.plot(df['DATE_TIME'], y, label='Data Aktual', marker='o', linestyle='-')
    plt.plot(df['DATE_TIME'], y_pred, label=f'Prediksi ({best_model_name})', marker='x', linestyle='--')
    plt.title(f'Prediksi vs Data Aktual untuk Sumur {well_name} (Model Terbaik: {best_model_name})')
    plt.xlabel('DATE_TIME')
    plt.ylabel(f'{well_name}_91_EP_NATURAL_GAS')
    plt.legend()
    plt.grid(True)
    plt.show()

# Iterate through each well, find the best model, and plot the results
for well_name, results in all_results.items():
    best_model_name = find_best_model(results)
    print(f"\nModel terbaik untuk Sumur {well_name}: {best_model_name}")

    # Get the model and scaler info for the best model
    best_model_info = results[best_model_name]

    # Plot the actual vs predicted for the best model
    plot_best_model_prediction(df, well_name, best_model_name, best_model_info)

# Import necessary libraries
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import xgboost as xgb
from sklearn.preprocessing import StandardScaler

# Create future dates for prediction
start_date = pd.to_datetime('2023-01-01')
end_date = pd.to_datetime('2023-12-31')
future_dates = pd.date_range(start=start_date, end=end_date, freq='D')
print("Generating predictions for period:", start_date.strftime('%Y-%m-%d'), "to", end_date.strftime('%Y-%m-%d'))

# Dictionary to store predictions for each well
predictions_dict = {}

# Train XGBoost model and make predictions for each well
for well_prefix in ['AA1', 'AA2', 'AA3', 'AA4', 'AA5']:
    print(f"\nTraining XGBoost model for {well_prefix}")

    # Select features and target for current well
    features = [f"{well_prefix}_91_9500062414", f"{well_prefix}_ANPR", f"{well_prefix}_ANTP",
                f"{well_prefix}_C_OP", f"{well_prefix}_C_PR", f"{well_prefix}_C_TE",
                f"{well_prefix}_FBHP", f"{well_prefix}_FBHT", f"{well_prefix}_FTHP",
                f"{well_prefix}_FTHT"]
    target = f"{well_prefix}_91_EP_NATURAL_GAS"

    X = df[features]
    y = df[target]

    # Scale the features
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)

    # Train XGBoost model on all data
    model = xgb.XGBRegressor(n_estimators=100, random_state=42)
    model.fit(X_scaled, y)

    # Generate synthetic future features based on historical patterns
    # Using mean values from last 30 days as a baseline
    last_30_days = X.tail(30).mean()

    # Create future features matrix
    n_days = len(future_dates)
    future_features = np.tile(last_30_days.values, (n_days, 1))

    # Add some random variation to make it more realistic
    np.random.seed(42)
    variation = np.random.normal(0, 0.1, future_features.shape)
    future_features = future_features * (1 + variation)

    # Scale future features
    future_features_scaled = scaler.transform(future_features)

    # Make predictions
    predictions = model.predict(future_features_scaled)

    # Store predictions
    predictions_dict[well_prefix] = predictions

# Create DataFrame with predictions
forecast_df = pd.DataFrame(index=future_dates)
for well_prefix in predictions_dict.keys():
    forecast_df[f"{well_prefix}_91_EP_NATURAL_GAS"] = predictions_dict[well_prefix]

# Save forecasts to CSV
forecast_df.to_csv('gas_production_forecast_2023.csv')
print("\nForecast summary:")
print(forecast_df.describe())

# Display first few rows of predictions
print("\nFirst few days of predictions:")
print(forecast_df.head())

df_predcited = pd.read_csv('gas_production_forecast_2023.csv')
df_predcited.head()

# prompt: Buat grafik persebaran untuk setiap waktu dari hasil gas produksi tiap sumur dari tahun 2023-01-01  sampai 2023-12-31

# Filter data for the specified date range
start_date = pd.to_datetime('2023-01-01')
end_date = pd.to_datetime('2023-12-31')
df_predcited['Unnamed: 0'] = pd.to_datetime(df_predcited['Unnamed: 0'])
forecast_df_filtered = df_predcited[(df_predcited['Unnamed: 0'] >= start_date) & (df_predcited['Unnamed: 0'] <= end_date)].copy()

# Rename the date column for clarity
forecast_df_filtered.rename(columns={'Unnamed: 0': 'DATE_TIME'}, inplace=True)

# Get the list of well columns (gas production)
well_columns = [col for col in forecast_df_filtered.columns if '_91_EP_NATURAL_GAS' in col]

# Create scatter plots for each well's gas production over time
plt.figure(figsize=(15, 10))

for i, well_col in enumerate(well_columns):
    plt.subplot(len(well_columns), 1, i + 1)
    sns.scatterplot(data=forecast_df_filtered, x='DATE_TIME', y=well_col, alpha=0.6)
    plt.title(f'Persebaran Produksi Gas {well_col} (2023)')
    plt.xlabel('Waktu')
    plt.ylabel('Produksi Gas')
    plt.grid(True)
    plt.tight_layout(rect=[0, 0.03, 1, 0.95]) # Adjust layout to prevent title overlap

plt.suptitle('Persebaran Produksi Gas Tiap Sumur dari 2023-01-01 hingga 2023-12-31', y=1.02)
plt.show()

